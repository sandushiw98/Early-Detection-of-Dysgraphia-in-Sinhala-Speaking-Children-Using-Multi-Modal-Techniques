{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS4WLRFjCaV1",
        "outputId": "ef572237-d3dc-4305-a36a-e1e65b52bb65"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OW_ZlJkzSypF"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/Research_Dysgraphia/ANN-Orginal.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/MyDrive/Research_Dysgraphia/ANN-Orginal.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "-IZJ2S6HS01o",
        "outputId": "11772249-ef25-4ac6-cde1-c30f4c08158c"
      },
      "outputs": [],
      "source": [
        "data.info()\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjy3JHZwS4nw",
        "outputId": "8eec1005-cc03-4868-9652-fee409f6cc41"
      },
      "outputs": [],
      "source": [
        "# Columns to impute with mean\n",
        "mean_impute_cols = [\n",
        "    'Age',\n",
        "    'Weight',\n",
        "    'OFC',\n",
        "    'Height',\n",
        "    'Birth Weight'\n",
        "]\n",
        "\n",
        "# Impute with mean where applicable\n",
        "for col in mean_impute_cols:\n",
        "    if col in data.columns:  # Check if the column exists in the DataFrame\n",
        "        mean_value = data[col].mean()\n",
        "        data[col].fillna(mean_value, inplace=True)\n",
        "\n",
        "# Impute rest of the columns with mode value\n",
        "for col in data.columns:\n",
        "    if col not in mean_impute_cols:  # Only apply to columns not in the mean_impute_cols\n",
        "        mode_value = data[col].mode().iloc[0]\n",
        "        data[col].fillna(mode_value, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNIgdcdjS9pd",
        "outputId": "028f8e0e-cd64-4aed-e197-a56825644c50"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Set options to display more rows and/or columns\n",
        "pd.set_option('display.max_columns', None)  # None means show all columns\n",
        "pd.set_option('display.max_rows', 100)      # Adjust number of rows to display\n",
        "\n",
        "# Now print the DataFrame\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1dEGivETBHE",
        "outputId": "27049534-88b8-4fc2-be8b-295ca1b6620d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Debugging print to check input data type\n",
        "print(data['Monthly Income'].apply(type))\n",
        "\n",
        "def calculate_average(income_range):\n",
        "    income_range = income_range.replace(',', '')  # Remove commas first\n",
        "    low, high = income_range.split('-')\n",
        "    return (int(low.strip()) + int(high.strip())) / 2\n",
        "\n",
        "def impute_monthly_income(value):\n",
        "    value = str(value).strip()  # Convert to string and remove spaces\n",
        "    print(f\"Processing value: {value}\")  # Debugging print to track values\n",
        "    if '-' in value:\n",
        "        return calculate_average(value)\n",
        "    elif '>' in value:\n",
        "        # Ensure no spaces between '>' and the number\n",
        "        return random.randint(50001, 100000)  # Generate a random value greater than 50,000\n",
        "    elif value.lower() == 'n/a':\n",
        "        return np.nan  # Convert 'N/A' to NaN\n",
        "    else:\n",
        "        return np.nan  # Return NaN for any other non-numeric or unhandled format\n",
        "\n",
        "\n",
        "# Apply the imputation function to the 'Monthly Income' column\n",
        "data['Monthly Income'] = data['Monthly Income'].apply(impute_monthly_income)\n",
        "\n",
        "# Display the first few rows of the data to check\n",
        "data.head()\n",
        "\n",
        "\n",
        "# Replace 'N/A' values with the mode of the column\n",
        "mode_value = data['Monthly Income'].dropna().mode()[0]  # Calculate mode excluding NaN\n",
        "data['Monthly Income'].fillna(mode_value, inplace=True)\n",
        "\n",
        "\n",
        "# Show the modified DataFrame\n",
        "print(data.head())  # Display the first few rows to verify changes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iukdXjMaTFNh",
        "outputId": "3b51452f-2b2a-4962-b00f-be993fa9bb60"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the mapping for the 'Mother' and 'Father' columns\n",
        "\n",
        "education_mapping = {\n",
        "    'Graduate': 1,\n",
        "    'A/L': 2,\n",
        "    'O/L': 3,\n",
        "    'Primary to O/L': 4,\n",
        "    'Primary': 5\n",
        "}\n",
        "\n",
        "# Apply the mappings\n",
        "\n",
        "data['Education_Mother'] = data['Education_Mother'].map(education_mapping)\n",
        "data['Education_Father'] = data['Education_Father'].map(education_mapping)\n",
        "\n",
        "\n",
        "print(data.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WTUA49MTKBu",
        "outputId": "6508761f-95dc-4bc7-a3f1-5b55e7f9cf6d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Define the weighted values for each feature\n",
        "\n",
        "eye_contact_mapping = {\n",
        "    'Eye Contact-Good': 1,\n",
        "    'Eye Contact_Fair': 2,\n",
        "    'Eye Contact_Poor': 3\n",
        "}\n",
        "gait_mapping = {\n",
        "    'Gait_Stable': 1,\n",
        "    'Gait_Ataxia': 2,\n",
        "    'Gait_Wide based': 3\n",
        "}\n",
        "\n",
        "activity_level_mapping = {\n",
        "    'Activity Level_Noraml': 1,\n",
        "    'Activity Level_Overactive': 2,\n",
        "    'Activity Level_Impulsive': 3\n",
        "}\n",
        "attention_mapping = {\n",
        "    'Attention_Inact ': 1,\n",
        "    'Attention_Limited': 2,\n",
        "    'Attention_Impaired': 3\n",
        "}\n",
        "behaviour_mapping = {\n",
        "    'Behaviour_Normal': 1,\n",
        "    'Behaviour_Uncooperative': 2,\n",
        "    'Behaviour_Anxious': 3,\n",
        "    'Behaviour_Disorganized': 4\n",
        "}\n",
        "speech_mapping = {\n",
        "    'Speech_Normal': 1,\n",
        "    'Speech_Slow': 3,\n",
        "    'Speech_Fast': 2,\n",
        "    'Speech_Disfluent': 4,\n",
        "    'Speech_Mute': 5,\n",
        "    'Speech_Stammering': 6,\n",
        "    'Speech_Prominal Reversal': 7\n",
        "}\n",
        "\n",
        "# Merge and map the columns\n",
        "data['Eye Contact'] = data[['Eye Contact-Good', 'Eye Contact_Fair', 'Eye Contact_Poor']].idxmax(axis=1).map(eye_contact_mapping)\n",
        "data['Gait'] = data[['Gait_Stable', 'Gait_Ataxia', 'Gait_Wide based']].idxmax(axis=1).map(gait_mapping)\n",
        "data['Activity Level'] = data[['Activity Level_Noraml', 'Activity Level_Overactive', 'Activity Level_Impulsive']].idxmax(axis=1).map(activity_level_mapping)\n",
        "data['Attention'] = data[['Attention_Inact ', 'Attention_Limited', 'Attention_Impaired']].idxmax(axis=1).map(attention_mapping)\n",
        "data['Behaviour'] = data[['Behaviour_Normal', 'Behaviour_Uncooperative', 'Behaviour_Anxious', 'Behaviour_Disorganized']].idxmax(axis=1).map(behaviour_mapping)\n",
        "data['Speech'] = data[['Speech_Normal', 'Speech_Slow', 'Speech_Fast', 'Speech_Disfluent', 'Speech_Mute', 'Speech_Stammering', 'Speech_Prominal Reversal']].idxmax(axis=1).map(speech_mapping)\n",
        "\n",
        "# Drop the original columns if no longer needed\n",
        "columns_to_drop = ['Eye Contact-Good', 'Eye Contact_Fair', 'Eye Contact_Poor', 'Gait_Stable', 'Gait_Ataxia', 'Gait_Wide based', 'Activity Level_Noraml', 'Activity Level_Overactive', 'Activity Level_Impulsive', 'Attention_Inact ', 'Attention_Limited', 'Attention_Impaired', 'Behaviour_Normal', 'Behaviour_Uncooperative', 'Behaviour_Anxious', 'Behaviour_Disorganized', 'Speech_Normal', 'Speech_Slow', 'Speech_Fast', 'Speech_Disfluent', 'Speech_Mute', 'Speech_Stammering', 'Speech_Prominal Reversal']\n",
        "data.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "# Ensure severity is the last column\n",
        "severity = data.pop('severity')\n",
        "data['severity'] = severity\n",
        "\n",
        "print(data.head())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdldM7GNTXfH",
        "outputId": "1525d09e-ab9e-41ae-8dfa-064d9cdadd4f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the weighted values for each feature\n",
        "expressive_language_mapping = {\n",
        "    'Expressive Language_Appropirate': 1,\n",
        "    'Expressive Language_Immature use of language': 2,\n",
        "    'Expressive Language_Primarily uses gestures': 3\n",
        "}\n",
        "\n",
        "\n",
        "reading_mapping = {\n",
        "    'Reading_Above Average': 1,\n",
        "    'Reading_Average': 3,\n",
        "    'Reading_Below Average': 5\n",
        "}\n",
        "\n",
        "writing_mapping = {\n",
        "    'Writing_Above Average': 1,\n",
        "    'Writing_Average': 3,\n",
        "    'Writing_Below Average': 5\n",
        "}\n",
        "\n",
        "mathematics_mapping = {\n",
        "    'Mathematics_Above Average': 1,\n",
        "    'Mathematics_Average': 3,\n",
        "    'Mathematics_Below Average': 5\n",
        "}\n",
        "\n",
        "# Functional skills mappings\n",
        "feeding_mapping = {\n",
        "    'Feeding_Independent': 1,\n",
        "    'Feeding_Needs support': 2,\n",
        "    'Feeding_Dependent': 3\n",
        "}\n",
        "\n",
        "bathing_mapping = {\n",
        "    'Bathing_Independent': 1,\n",
        "    'Bathing_Needs support': 2,\n",
        "    'Bathing_Dependent': 3\n",
        "}\n",
        "\n",
        "mobility_mapping = {\n",
        "    'Mobility_Independent': 1,\n",
        "    'Mobility_Needs support': 2,\n",
        "    'Mobility_Dependent': 3\n",
        "}\n",
        "\n",
        "dressing_mapping = {\n",
        "    'Dressing_Independent': 1,\n",
        "    'Dressing_Needs support': 2,\n",
        "    'Dressing_Dependent': 3\n",
        "}\n",
        "\n",
        "continence_mapping = {\n",
        "    'Continence_Independent': 1,\n",
        "    'Continence_Needs support': 2,\n",
        "    'Continence_Dependent': 3\n",
        "}\n",
        "\n",
        "# Other Skills mappings\n",
        "self_care_mapping = {\n",
        "    'Self-care_Satisafactory': 1,\n",
        "    'Self-care_Unsatisfactory': 2\n",
        "}\n",
        "\n",
        "home_living_mapping = {\n",
        "    'Home-living_Satisafactory': 1,\n",
        "    'Home-living_Unsatisfactory': 2\n",
        "}\n",
        "\n",
        "social_skills_mapping = {\n",
        "    'Social-skills_Satisafactory': 1,\n",
        "    'Social-skills_Unsatisfactory': 2\n",
        "}\n",
        "\n",
        "safety_mapping = {\n",
        "    'Safety_Satisafactory': 1,\n",
        "    'Safety_Unsatisfactory': 2\n",
        "}\n",
        "\n",
        "leisure_activities_mapping = {\n",
        "    'Leisure Activites_Satisafactory': 1,\n",
        "    'Leisure Activites_Unsatisfactory': 2\n",
        "}\n",
        "\n",
        "# Assuming your data DataFrame contains all these columns, the below code merges and maps them\n",
        "\n",
        "# Mapping Expressive Language, IQ, Reading, Writing, Mathematics and others\n",
        "data['Expressive Language'] = data[['Expressive Language_Appropirate', 'Expressive Language_Immature use of language', 'Expressive Language_Primarily uses gestures']].idxmax(axis=1).map(expressive_language_mapping)\n",
        "data['Reading'] = data[['Reading_Above Average', 'Reading_Average', 'Reading_Below Average']].idxmax(axis=1).map(reading_mapping)\n",
        "data['Writing'] = data[['Writing_Above Average', 'Writing_Average', 'Writing_Below Average']].idxmax(axis=1).map(writing_mapping)\n",
        "data['Mathematics'] = data[['Mathematics_Above Average', 'Mathematics_Average', 'Mathematics_Below Average']].idxmax(axis=1).map(mathematics_mapping)\n",
        "\n",
        "# Mapping functional skills\n",
        "data['Feeding'] = data[['Feeding_Independent', 'Feeding_Needs support', 'Feeding_Dependent']].idxmax(axis=1).map(feeding_mapping)\n",
        "data['Bathing'] = data[['Bathing_Independent', 'Bathing_Needs support', 'Bathing_Dependent']].idxmax(axis=1).map(bathing_mapping)\n",
        "data['Mobility'] = data[['Mobility_Independent', 'Mobility_Needs support', 'Mobility_Dependent']].idxmax(axis=1).map(mobility_mapping)\n",
        "data['Dressing'] = data[['Dressing_Independent', 'Dressing_Needs support', 'Dressing_Dependent']].idxmax(axis=1).map(dressing_mapping)\n",
        "data['Continence'] = data[['Continence_Independent', 'Continence_Needs support', 'Continence_Dependent']].idxmax(axis=1).map(continence_mapping)\n",
        "\n",
        "# Mapping other skills\n",
        "data['Self-care'] = data[['Self-care_Satisafactory', 'Self-care_Unsatisfactory']].idxmax(axis=1).map(self_care_mapping)\n",
        "data['Home-living'] = data[['Home-living_Satisafactory', 'Home-living_Unsatisfactory']].idxmax(axis=1).map(home_living_mapping)\n",
        "data['Social-skills'] = data[['Social-skills_Satisafactory', 'Social-skills_Unsatisfactory']].idxmax(axis=1).map(social_skills_mapping)\n",
        "data['Safety'] = data[['Safety_Satisafactory', 'Safety_Unsatisfactory']].idxmax(axis=1).map(safety_mapping)\n",
        "data['Leisure Activites'] = data[['Leisure Activites_Satisafactory','Leisure Activites_Unsatisfactory']].idxmax(axis=1).map(leisure_activities_mapping)\n",
        "\n",
        "# Drop the original columns if no longer needed\n",
        "columns_to_drop = ['Expressive Language_Appropirate', 'Expressive Language_Immature use of language', 'Expressive Language_Primarily uses gestures',\n",
        "                   'Reading_Above Average', 'Reading_Average', 'Reading_Below Average',\n",
        "                   'Writing_Above Average', 'Writing_Average', 'Writing_Below Average',\n",
        "                   'Mathematics_Above Average', 'Mathematics_Average', 'Mathematics_Below Average',\n",
        "                   'Feeding_Independent', 'Feeding_Needs support', 'Feeding_Dependent',\n",
        "                   'Bathing_Independent', 'Bathing_Needs support', 'Bathing_Dependent',\n",
        "                   'Mobility_Independent', 'Mobility_Needs support', 'Mobility_Dependent',\n",
        "                   'Dressing_Independent', 'Dressing_Needs support', 'Dressing_Dependent',\n",
        "                   'Continence_Independent', 'Continence_Needs support', 'Continence_Dependent',\n",
        "                   'Self-care_Satisafactory', 'Self-care_Unsatisfactory',\n",
        "                   'Home-living_Satisafactory', 'Home-living_Unsatisfactory',\n",
        "                   'Social-skills_Satisafactory', 'Social-skills_Unsatisfactory',\n",
        "                   'Safety_Satisafactory', 'Safety_Unsatisfactory','Leisure Activites_Satisafactory','Leisure Activites_Unsatisfactory'\n",
        "]\n",
        "\n",
        "\n",
        "data.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "# Ensure severity is the last column\n",
        "severity = data.pop('severity')\n",
        "data['severity'] = severity\n",
        "\n",
        "print(data.info())\n",
        "print(data.head())\n",
        "\n",
        "# Save the modified DataFrame\n",
        "data.to_csv('/content/drive/MyDrive/Research_Dysgraphia/ANN_After_Feature_Engineering.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot9YPJOI_gRX",
        "outputId": "767a5b47-187f-4bb3-9216-0ed536809f61"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_After_Feature_Engineering.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "# Initialize MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Select columns to scale (excluding 'Child ID' and already binary columns)\n",
        "columns_to_scale = data.columns.drop('Child ID')  # Adjust this to exclude any other non-continuous columns\n",
        "\n",
        "# Identify columns that are not already 0 or 1\n",
        "columns_to_scale = [col for col in columns_to_scale if not all(data[col].isin([0, 1]))]\n",
        "\n",
        "# Apply the scaler to these columns\n",
        "data[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])\n",
        "\n",
        "# Round the scaled data to two decimal places\n",
        "data[columns_to_scale] = data[columns_to_scale].round(2)\n",
        "\n",
        "# Save the normalized and rounded data\n",
        "normalized_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_Normalized_data.csv'\n",
        "data.to_csv(normalized_file_path, index=False)\n",
        "\n",
        "print(f\"Normalized data saved to {normalized_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oUfW48H9y0ji",
        "outputId": "f24d79da-0b75-467d-a0fc-ccae2ef0b5bc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the normalized data\n",
        "normalized_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_Normalized_data.csv'\n",
        "print(f\"Loading data from {normalized_file_path}\")\n",
        "data = pd.read_csv(normalized_file_path)\n",
        "print(\"Data loaded successfully. Data shape:\", data.shape)\n",
        "\n",
        "# Drop the 'Child ID' column as it is not relevant for the correlation analysis\n",
        "print(\"Dropping 'Child ID' column\")\n",
        "data = data.drop('Child ID', axis=1)\n",
        "print(\"'Child ID' column dropped. Data shape:\", data.shape)\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "print(\"Calculating correlation matrix\")\n",
        "corr_matrix = data.corr()\n",
        "#spearman_corr = data.corr(method='spearman')\n",
        "print(\"Correlation matrix calculated successfully\")\n",
        "\n",
        "# Extract correlation with 'Severity' specifically\n",
        "print(\"Extracting correlation with 'Severity'\")\n",
        "severity_corr = corr_matrix[['severity']]\n",
        "print(\"Correlation with 'Severity' extracted. Shape:\", severity_corr.shape)\n",
        "\n",
        "# Split the features into batches of 20 for better visualization\n",
        "num_features = len(severity_corr)\n",
        "features_per_batch = 20\n",
        "print(f\"Total features: {num_features}, Features per batch: {features_per_batch}\")\n",
        "\n",
        "# Loop through the features in batches of 20\n",
        "for i in range(0, num_features, features_per_batch):\n",
        "    batch = severity_corr.iloc[i:i + features_per_batch]\n",
        "    print(f\"Processing batch {i // features_per_batch + 1} with features {i} to {i + len(batch) - 1}\")\n",
        "\n",
        "    # Set up the matplotlib figure\n",
        "    plt.figure(figsize=(4, len(batch) * 0.5))  # Adjust the figure size to fit the batch size\n",
        "\n",
        "    # Draw the heatmap with the mask and correct aspect ratio\n",
        "    sns.heatmap(batch, annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1,\n",
        "                square=False, linewidths=.5, cbar_kws={\"shrink\": .8})\n",
        "\n",
        "    # Adjust the plot to make sure all is visible\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.yticks(rotation=0, ha='right')\n",
        "    plt.title(f'Correlation with Severity - Batch {i // features_per_batch + 1}')\n",
        "\n",
        "    # Show the plot\n",
        "    print(f\"Displaying heatmap for batch {i // features_per_batch + 1}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7yx4-qA6qIx",
        "outputId": "327d5bfd-755d-473c-8d07-1deccc1772cb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the data from the specified path\n",
        "file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_Feature Selection.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert 'severity' to categorical bins\n",
        "bins = [-np.inf, 0.75, np.inf]\n",
        "labels = ['Low', 'High']\n",
        "data['severity'] = pd.cut(data['severity'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "# Save the modified data to the specified path\n",
        "modified_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_modified_data.csv'\n",
        "data.to_csv(modified_file_path, index=False)\n",
        "\n",
        "print(\"Data processing completed and saved to:\", modified_file_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBtIHbSdD7wU",
        "outputId": "34d75589-825a-4c8b-e2de-538ed74ce932"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the modified data from the specified path\n",
        "modified_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_modified_data.csv'\n",
        "data = pd.read_csv(modified_file_path)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save the training and testing datasets to specified paths\n",
        "train_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_train_data.csv'\n",
        "test_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_test_data.csv'\n",
        "\n",
        "train_data.to_csv(train_file_path, index=False)\n",
        "test_data.to_csv(test_file_path, index=False)\n",
        "\n",
        "print(\"Data split completed and saved to:\")\n",
        "print(\"Training data:\", train_file_path)\n",
        "print(\"Testing data:\", test_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKY80wCAlphi"
      },
      "source": [
        "# RandoM Forest-SMOTE Before Cross Validation\n",
        "**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g88LdpExlnCf",
        "outputId": "65daaa48-0a04-4e5b-f030-5e8b86b97403"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Load the modified data from the specified path\n",
        "modified_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_modified_data.csv'\n",
        "data = pd.read_csv(modified_file_path)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data.iloc[:, -1])\n",
        "\n",
        "# Save the training and testing datasets to specified paths\n",
        "train_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_train_data.csv'\n",
        "test_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_test_data.csv'\n",
        "\n",
        "train_data.to_csv(train_file_path, index=False)\n",
        "test_data.to_csv(test_file_path, index=False)\n",
        "\n",
        "print(\"Data split completed and saved to:\")\n",
        "print(\"Training data:\", train_file_path)\n",
        "print(\"Testing data:\", test_file_path)\n",
        "\n",
        "# Assume the last column is the target variable, and the rest are features\n",
        "X = train_data.iloc[:, :-1]  # Features\n",
        "y = train_data.iloc[:, -1]   # Target\n",
        "\n",
        "# Split dataset into features and target for training\n",
        "X_train = X\n",
        "y_train = y\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# Apply SMOTE to handle the data imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Display the shape of the resampled dataset\n",
        "print(\"Before SMOTE:\", X_train.shape, y_train.value_counts())\n",
        "print(\"After SMOTE:\", X_train_resampled.shape, pd.Series(y_train_resampled).value_counts())\n",
        "\n",
        "# Train a Random Forest Classifier with k-fold cross-validation\n",
        "rfc = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        "\n",
        "# Define Stratified K-Fold Cross Validator\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cross_val_scores = cross_val_score(rfc, X_train_resampled, y_train_resampled, cv=kf, scoring='accuracy')\n",
        "\n",
        "# Display cross-validation results\n",
        "print(\"Cross-Validation Accuracy Scores:\", cross_val_scores)\n",
        "print(\"Mean Cross-Validation Accuracy:\", np.mean(cross_val_scores))\n",
        "\n",
        "# Train on the entire resampled training set\n",
        "rfc.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Load the testing data\n",
        "X_test = test_data.iloc[:, :-1]  # Features\n",
        "y_test = test_data.iloc[:, -1]   # Target\n",
        "\n",
        "# Standardize the test features\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rfc.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the model to a file\n",
        "joblib.dump(rfc, '/content/drive/MyDrive/Research_Dysgraphia/random_forest_pipeline.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRpkAg_VTO0h"
      },
      "source": [
        "# Random Forest-SMOTE during cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXaaLx_3TEni",
        "outputId": "395f9d10-c61e-4081-cda1-a53a3f4f8b91"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.pipeline import make_pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "modified_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_modified_data.csv'\n",
        "data = pd.read_csv(modified_file_path)\n",
        "\n",
        "# Assume the last column is the target variable, and the rest are features\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    SMOTE(random_state=42),\n",
        "    RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        ")\n",
        "\n",
        "# Define Stratified K-Fold Cross Validator\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cross_val_scores = cross_val_score(\n",
        "    pipeline, X_train_full, y_train_full, cv=kf, scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Display cross-validation results\n",
        "print(\"Cross-Validation Accuracy Scores:\", cross_val_scores)\n",
        "print(\"Mean Cross-Validation Accuracy:\", np.mean(cross_val_scores))\n",
        "\n",
        "# Fit the pipeline on the entire training data\n",
        "pipeline.fit(X_train_full, y_train_full)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy on Test Set:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report on Test Set:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the model to an HDF5 file\n",
        "import joblib\n",
        "\n",
        "# Save the pipeline\n",
        "joblib.dump(pipeline, '/content/drive/MyDrive/Research_Dysgraphia/random_forest_pipeline.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeDUQAEMlwoY"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_U8gdaWlHLF"
      },
      "source": [
        "# SMOTE before the cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP2bC9NpwCWO",
        "outputId": "b7315f96-b97a-40d0-bf20-d836ace53581"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Load the modified data from the specified path\n",
        "modified_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_modified_data.csv'\n",
        "data = pd.read_csv(modified_file_path)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data.iloc[:, -1])\n",
        "\n",
        "# Save the training and testing datasets to specified paths\n",
        "train_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_train_data.csv'\n",
        "test_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_test_data.csv'\n",
        "\n",
        "train_data.to_csv(train_file_path, index=False)\n",
        "test_data.to_csv(test_file_path, index=False)\n",
        "\n",
        "print(\"Data split completed and saved to:\")\n",
        "print(\"Training data:\", train_file_path)\n",
        "print(\"Testing data:\", test_file_path)\n",
        "\n",
        "# Assume the last column is the target variable, and the rest are features\n",
        "X = train_data.iloc[:, :-1]  # Features\n",
        "y = train_data.iloc[:, -1]   # Target\n",
        "\n",
        "# Split dataset into features and target for training\n",
        "X_train = X\n",
        "y_train = y\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# Apply SMOTE to handle the data imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Display the shape of the resampled dataset\n",
        "print(\"Before SMOTE:\", X_train.shape, y_train.value_counts())\n",
        "print(\"After SMOTE:\", X_train_resampled.shape, pd.Series(y_train_resampled).value_counts())\n",
        "\n",
        "# Train an SVM with k-fold cross-validation\n",
        "svm = SVC(kernel='rbf', class_weight='balanced', random_state=42)\n",
        "\n",
        "# Define Stratified K-Fold Cross Validator\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cross_val_scores = cross_val_score(svm, X_train_resampled, y_train_resampled, cv=kf, scoring='accuracy')\n",
        "\n",
        "# Display cross-validation results\n",
        "print(\"Cross-Validation Accuracy Scores:\", cross_val_scores)\n",
        "print(\"Mean Cross-Validation Accuracy:\", np.mean(cross_val_scores))\n",
        "\n",
        "# Train on the entire resampled training set\n",
        "svm.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Load the testing data\n",
        "X_test = test_data.iloc[:, :-1]  # Features\n",
        "y_test = test_data.iloc[:, -1]   # Target\n",
        "\n",
        "# Standardize the test features\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Save the model to a file\n",
        "joblib.dump(svm, '/content/drive/MyDrive/Research_Dysgraphia/svm_pipeline.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llzhQhNvs_4v"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aSw8cQNixHt",
        "outputId": "3ba8355f-9b18-490e-a5fb-c57fcdb6090d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Load the testing data from the specified path\n",
        "test_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_test_data.csv'\n",
        "test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "# Load the saved model\n",
        "model_path = '/content/drive/MyDrive/Research_Dysgraphia/svm_pipeline.pkl'\n",
        "svm = joblib.load(model_path)\n",
        "\n",
        "# Assume the last column is the target variable, and the rest are features\n",
        "X_test = test_data.iloc[:, :-1]  # Features\n",
        "y_test = test_data.iloc[:, -1]   # Target\n",
        "\n",
        "# Standardize the test features using the same parameters from training\n",
        "scaler = StandardScaler()\n",
        "X_train_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_train_data.csv'\n",
        "train_data = pd.read_csv(X_train_file_path)\n",
        "X_train = train_data.iloc[:, :-1]\n",
        "scaler.fit(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Testing Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R39Ili_hlOJt"
      },
      "source": [
        "# SMOTE During the cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skT6qnYWUGP8",
        "outputId": "f77132bb-8ef0-48ff-d15c-1a8d56c9e123"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.pipeline import make_pipeline\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Load training dataset\n",
        "train_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_train_data.csv'\n",
        "data = pd.read_csv(train_file_path)\n",
        "\n",
        "# Assume the last column is the target variable, and the rest are features\n",
        "X_train_full = data.iloc[:, :-1]  # Features\n",
        "y_train_full = data.iloc[:, -1]   # Target\n",
        "\n",
        "# Define the pipeline with reduced SMOTE sampling strategy\n",
        "pipeline = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    SMOTE(random_state=42, sampling_strategy=0.5),  # Reduce SMOTE over-sampling\n",
        "    SVC(kernel='rbf', class_weight='balanced', random_state=42, C=0.5)  # Increase regularization strength\n",
        ")\n",
        "\n",
        "# Define Stratified K-Fold Cross Validator\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store fold-wise metrics\n",
        "fold_var = 1\n",
        "accuracies = []\n",
        "\n",
        "# Perform K-Fold Cross-Validation manually to track fold-wise metrics\n",
        "for train_index, val_index in kf.split(X_train_full, y_train_full):\n",
        "    # Split the data into training and validation sets for the current fold\n",
        "    X_train, X_val = X_train_full.iloc[train_index], X_train_full.iloc[val_index]\n",
        "    y_train, y_val = y_train_full.iloc[train_index], y_train_full.iloc[val_index]\n",
        "\n",
        "    # Fit the pipeline on the training data of the current fold\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_val_pred = pipeline.predict(X_val)\n",
        "\n",
        "    # Calculate accuracy for the current fold\n",
        "    accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    # Display fold-wise metrics\n",
        "    print(f\"Fold {fold_var} - Accuracy: {accuracy}\")\n",
        "    print(f\"Fold {fold_var} - Classification Report:\\n{classification_report(y_val, y_val_pred)}\")\n",
        "\n",
        "    # Save the pipeline for the current fold\n",
        "    joblib.dump(pipeline, f'/content/drive/MyDrive/Research_Dysgraphia/svm_pipeline_fold{fold_var}.pkl')\n",
        "\n",
        "    # Increment the fold counter\n",
        "    fold_var += 1\n",
        "\n",
        "# Display cross-validation results\n",
        "print(\"Cross-Validation Accuracy Scores:\", accuracies)\n",
        "print(\"Mean Cross-Validation Accuracy:\", np.mean(accuracies))\n",
        "\n",
        "# Fit the pipeline on the entire training data\n",
        "pipeline.fit(X_train_full, y_train_full)\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_pred = pipeline.predict(X_train_full)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report on Training Set:\\n\", classification_report(y_train_full, y_pred))\n",
        "\n",
        "# Save the final pipeline to a file\n",
        "joblib.dump(pipeline, '/content/drive/MyDrive/Research_Dysgraphia/svm_pipeline_final.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT6nPnBctFCm"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxCoLGZsn0QK",
        "outputId": "32cccfa8-dcf9-4cf7-8b0e-12216282226f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Load testing dataset\n",
        "test_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_test_data.csv'\n",
        "test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "# Assume the last column is the target variable, and the rest are features\n",
        "X_test = test_data.iloc[:, :-1]  # Features\n",
        "y_test = test_data.iloc[:, -1]   # Target\n",
        "\n",
        "# Load the trained pipeline\n",
        "pipeline = joblib.load(f'/content/drive/MyDrive/Research_Dysgraphia/svm_pipeline_fold{3}.pkl')\n",
        "\n",
        "# Standardize the test data using the scaler from the trained pipeline\n",
        "scaler = pipeline.named_steps['standardscaler']\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_test_pred = pipeline.named_steps['svc'].predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model on testing set\n",
        "print(\"Accuracy on Test Set:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"Classification Report on Test Set:\\n\", classification_report(y_test, y_test_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "8ZmMx3GO8tVP",
        "outputId": "b717548b-11ff-4571-8548-52af3133cf65"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Cross-validation accuracies from the previous code\n",
        "accuracies = [0.85, 0.88, 0.84, 0.86, 0.87]  # Replace with your accuracies list if different\n",
        "\n",
        "# Assuming these are similar to training and validation metrics over epochs\n",
        "val_accuracies = [0.83, 0.87, 0.82, 0.85, 0.86]  # Example validation accuracies\n",
        "train_losses = [0.4, 0.35, 0.3, 0.28, 0.25]  # Example training losses\n",
        "val_losses = [0.42, 0.37, 0.33, 0.3, 0.27]  # Example validation losses\n",
        "\n",
        "# Plotting Accuracy Over Folds\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Accuracy Plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(accuracies) + 1), accuracies, marker='o', linestyle='-', color='b', label='Training Accuracy')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, marker='o', linestyle='--', color='g', label='Validation Accuracy')\n",
        "plt.xlabel('Fold Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Over Folds')\n",
        "plt.xticks(range(1, len(accuracies) + 1))\n",
        "plt.ylim([0, 1])\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Loss Plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', linestyle='-', color='b', label='Training Loss')\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, marker='o', linestyle='--', color='g', label='Validation Loss')\n",
        "plt.xlabel('Fold Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Over Folds')\n",
        "plt.xticks(range(1, len(train_losses) + 1))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6ghgyCAC8gt",
        "outputId": "93d4967b-582e-48aa-fd73-bdcb1d5e049b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.pipeline import make_pipeline\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Load training dataset\n",
        "train_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_train_data.csv'\n",
        "data = pd.read_csv(train_file_path)\n",
        "\n",
        "# Assume the last column is the target variable, and the rest are features\n",
        "X_train_full = data.iloc[:, :-1]  # Features\n",
        "y_train_full = data.iloc[:, -1]   # Target\n",
        "\n",
        "# Define the pipeline with reduced SMOTE sampling strategy\n",
        "pipeline = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    SMOTE(random_state=42, sampling_strategy=0.5),  # Reduce SMOTE over-sampling\n",
        "    GradientBoostingClassifier(random_state=42, n_estimators=50, learning_rate=0.05, max_depth=2)  # Reduced complexity to prevent overfitting\n",
        ")\n",
        "\n",
        "# Define Stratified K-Fold Cross Validator\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store fold-wise metrics\n",
        "fold_var = 1\n",
        "accuracies = []\n",
        "\n",
        "# Perform K-Fold Cross-Validation manually to track fold-wise metrics\n",
        "for train_index, val_index in kf.split(X_train_full, y_train_full):\n",
        "    # Split the data into training and validation sets for the current fold\n",
        "    X_train, X_val = X_train_full.iloc[train_index], X_train_full.iloc[val_index]\n",
        "    y_train, y_val = y_train_full.iloc[train_index], y_train_full.iloc[val_index]\n",
        "\n",
        "    # Fit the pipeline on the training data of the current fold\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_val_pred = pipeline.predict(X_val)\n",
        "\n",
        "    # Calculate accuracy for the current fold\n",
        "    accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    # Display fold-wise metrics\n",
        "    print(f\"Fold {fold_var} - Accuracy: {accuracy}\")\n",
        "    print(f\"Fold {fold_var} - Classification Report:\\n{classification_report(y_val, y_val_pred)}\")\n",
        "\n",
        "    # Save the pipeline for the current fold\n",
        "    joblib.dump(pipeline, f'/content/drive/MyDrive/Research_Dysgraphia/gb_pipeline_fold{fold_var}.pkl')\n",
        "\n",
        "    # Increment the fold counter\n",
        "    fold_var += 1\n",
        "\n",
        "# Display cross-validation results\n",
        "print(\"Cross-Validation Accuracy Scores:\", accuracies)\n",
        "print(\"Mean Cross-Validation Accuracy:\", np.mean(accuracies))\n",
        "\n",
        "# Fit the pipeline on the entire training data\n",
        "pipeline.fit(X_train_full, y_train_full)\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_pred = pipeline.predict(X_train_full)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification Report on Training Set:\\n\", classification_report(y_train_full, y_pred))\n",
        "\n",
        "# Save the final pipeline to a file\n",
        "joblib.dump(pipeline, '/content/drive/MyDrive/Research_Dysgraphia/gb_pipeline_final.pkl')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPRvZtqxDJml",
        "outputId": "3578318c-3f88-4adc-ac38-5e26c2d5b2eb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import joblib\n",
        "\n",
        "# Load testing dataset\n",
        "test_file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_test_data.csv'\n",
        "test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "# Assume the last column is the target variable, and the rest are features\n",
        "X_test = test_data.iloc[:, :-1]  # Features\n",
        "y_test = test_data.iloc[:, -1]   # Target\n",
        "\n",
        "# Load the trained pipeline\n",
        "pipeline = joblib.load(f'/content/drive/MyDrive/Research_Dysgraphia/gb_pipeline_fold{1}.pkl')\n",
        "\n",
        "# Standardize the test data using the scaler from the trained pipeline\n",
        "scaler = pipeline.named_steps['standardscaler']\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_test_pred = pipeline.named_steps['gradientboostingclassifier'].predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model on testing set\n",
        "print(\"Accuracy on Test Set:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"Classification Report on Test Set:\\n\", classification_report(y_test, y_test_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL-6voC5YV56"
      },
      "source": [
        "# Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqs1_dv5rRwX"
      },
      "source": [
        "## Using SMOTE Inside Each Cross-Validation Fold\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64raNrXCBRvy"
      },
      "source": [
        "Training Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ToHXCuqUJOz",
        "outputId": "53a92b44-863c-4245-df62-f5132745a16d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load your data\n",
        "file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_modified_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "X = data.drop('severity', axis=1)\n",
        "y = data['severity']\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Define cross-validator and hyperparameter grid for MLPClassifier\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(10,), (30,), (30, 10)],  # Smaller architectures\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam', 'lbfgs'],\n",
        "    'alpha': [0.01, 0.1, 1.0, 10.0],  # Stronger regularization values\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'max_iter': [100, 200],\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize the MLP classifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, early_stopping=True, random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=skf, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Lists to store results\n",
        "metrics_list = []\n",
        "\n",
        "# Perform cross-validation\n",
        "fold = 0\n",
        "for train_index, test_index in skf.split(X_scaled, y):\n",
        "    fold += 1\n",
        "    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "\n",
        "    # Apply ADASYN (SMOTE alternative) on each fold training data\n",
        "    smote = ADASYN(sampling_strategy='auto', random_state=42)\n",
        "    X_train_fold_smote, y_train_fold_smote = smote.fit_resample(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Perform grid search to find the best hyperparameters\n",
        "    grid_search.fit(X_train_fold_smote, y_train_fold_smote)\n",
        "\n",
        "    # Train the classifier with the best hyperparameters found\n",
        "    best_classifier = grid_search.best_estimator_\n",
        "\n",
        "    # Predict on the training fold (training accuracy)\n",
        "    y_train_pred = best_classifier.predict(X_train_fold_smote)\n",
        "\n",
        "    # Calculate training accuracy and other metrics\n",
        "    accuracy_train = accuracy_score(y_train_fold_smote, y_train_pred)\n",
        "    report_train = classification_report(y_train_fold_smote, y_train_pred, target_names=['High', 'Low'], output_dict=True)\n",
        "\n",
        "    # Store metrics for each fold\n",
        "    metrics = {\n",
        "        'Fold': fold,\n",
        "        'Training Accuracy': accuracy_train,\n",
        "        'High Precision': report_train['High']['precision'],\n",
        "        'High Recall': report_train['High']['recall'],\n",
        "        'High F1': report_train['High']['f1-score'],\n",
        "        'Low Precision': report_train['Low']['precision'],\n",
        "        'Low Recall': report_train['Low']['recall'],\n",
        "        'Low F1': report_train['Low']['f1-score'],\n",
        "    }\n",
        "    metrics_list.append(metrics)\n",
        "\n",
        "    # Print training metrics for the current fold\n",
        "    print(f\"Training Metrics for Fold {fold}:\")\n",
        "    print(pd.DataFrame([metrics]))\n",
        "\n",
        "    # Print the best hyperparameters for the current fold\n",
        "    print(f\"Best hyperparameters for Fold {fold}: {grid_search.best_params_}\")\n",
        "\n",
        "# Summarize training metrics across all folds\n",
        "all_metrics_df = pd.DataFrame(metrics_list)\n",
        "print(\"\\nSummary of Training Metrics Across All Folds:\")\n",
        "print(all_metrics_df.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q8xpb_kA601"
      },
      "source": [
        "Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCjFvLLqAyvx",
        "outputId": "44a87282-5b72-4449-ed76-bdab67845f13"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load your data\n",
        "file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_modified_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "X = data.drop('severity', axis=1)\n",
        "y = data['severity']\n",
        "\n",
        "# Step 1: Split the data into training (80%) and test (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Step 2: Normalize the features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 3: Apply ADASYN (SMOTE alternative) to the training data only\n",
        "smote = ADASYN(sampling_strategy='auto', random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Step 4: Define cross-validator and hyperparameter grid for MLPClassifier\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 30), (100, 50)],  # Reduce model complexity\n",
        "    'activation': ['relu', 'tanh'],  # Activation functions\n",
        "    'solver': ['adam', 'lbfgs'],  # Solvers for weight optimization\n",
        "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0],  # Increase regularization values\n",
        "    'learning_rate': ['constant', 'adaptive'],  # Learning rate strategies\n",
        "    'max_iter': [100, 200, 300],  # Reduce the maximum number of iterations\n",
        "}\n",
        "\n",
        "# Initialize the MLP classifier with early stopping\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, early_stopping=True, random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=skf, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Step 5: Perform grid search cross-validation on the training set with SMOTE\n",
        "grid_search.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Step 6: After cross-validation, get the best model\n",
        "best_classifier = grid_search.best_estimator_\n",
        "\n",
        "# Step 7: Evaluate the best model on the unseen test data\n",
        "y_test_pred = best_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy and other metrics on the test set\n",
        "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
        "report_test = classification_report(y_test, y_test_pred, target_names=['High', 'Low'])\n",
        "\n",
        "# Print the final test accuracy and classification report\n",
        "print(f\"\\nTest Accuracy on the unseen test data: {accuracy_test}\")\n",
        "print(\"Classification Report on the test data:\")\n",
        "print(report_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfoMEgMgqb8W"
      },
      "source": [
        "# Applying SMOTE Before Cross-Validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCxXogsC3ixz"
      },
      "source": [
        "Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jbUgLtGcoJv",
        "outputId": "44632569-295e-4dd4-c1a2-7f268dce24d7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load your data\n",
        "file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_modified_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "X = data.drop('severity', axis=1)\n",
        "y = data['severity']\n",
        "\n",
        "# Split the data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Normalize the features for both training and test sets\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Apply SMOTE to the training set (not the test set)\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Option 1: Use cross-validation on the training set\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, activation='relu', random_state=42)\n",
        "\n",
        "# Lists to store results\n",
        "metrics_list = []\n",
        "\n",
        "# Perform cross-validation on the training data (with SMOTE)\n",
        "fold = 0\n",
        "for train_index, test_index in skf.split(X_train_smote, y_train_smote):\n",
        "    fold += 1\n",
        "    X_train_fold, X_val_fold = X_train_smote[train_index], X_train_smote[test_index]\n",
        "    y_train_fold, y_val_fold = y_train_smote[train_index], y_train_smote[test_index]\n",
        "\n",
        "    # Train classifier on the training fold\n",
        "    classifier.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Predict on the validation fold\n",
        "    y_pred_val = classifier.predict(X_val_fold)\n",
        "\n",
        "    # Calculate metrics on the validation fold\n",
        "    accuracy = accuracy_score(y_val_fold, y_pred_val)\n",
        "    report = classification_report(y_val_fold, y_pred_val, target_names=['High', 'Low'], output_dict=True)\n",
        "\n",
        "    # Store metrics for each fold\n",
        "    metrics = {\n",
        "        'Fold': fold,\n",
        "        'Overall Accuracy': accuracy,\n",
        "        'High Precision': report['High']['precision'],\n",
        "        'High Recall': report['High']['recall'],\n",
        "        'High F1': report['High']['f1-score'],\n",
        "        'Low Precision': report['Low']['precision'],\n",
        "        'Low Recall': report['Low']['recall'],\n",
        "        'Low F1': report['Low']['f1-score'],\n",
        "    }\n",
        "    metrics_list.append(metrics)\n",
        "\n",
        "    # Print metrics for current fold\n",
        "    print(f\"Metrics for Fold {fold}:\")\n",
        "    print(pd.DataFrame([metrics]))\n",
        "\n",
        "# Summarize all folds metrics\n",
        "all_metrics_df = pd.DataFrame(metrics_list)\n",
        "print(\"\\nSummary of Metrics Across All Folds (Cross-Validation on Training Set):\")\n",
        "print(all_metrics_df.mean())\n",
        "\n",
        "# --- Final evaluation on the unseen test data (20% test set) ---\n",
        "# Train the classifier on the entire training set (with SMOTE)\n",
        "classifier.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_test = classifier.predict(X_test_scaled)\n",
        "\n",
        "# Calculate metrics on the test set\n",
        "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "report_test = classification_report(y_test, y_pred_test, target_names=['High', 'Low'])\n",
        "\n",
        "# Print the final metrics for the test set\n",
        "print(f\"\\nAccuracy on test data (unseen 20%): {accuracy_test}\")\n",
        "print(\"Classification Report on test data:\")\n",
        "print(report_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fEZ9vFQ3fuT"
      },
      "source": [
        "Test data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFVQ6zR9qKS2",
        "outputId": "54f5e142-7d30-4a04-ff20-5ad4d3197241"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load your data\n",
        "file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_modified_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "X = data.drop('severity', axis=1)  # Features\n",
        "y = data['severity']  # Target variable\n",
        "\n",
        "# Split the data into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)  # Important to use the same scaler for the test set\n",
        "\n",
        "# Apply SMOTE to the training set (not the test set)\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Train the MLPClassifier on the SMOTE-applied training set\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, activation='relu', random_state=42)\n",
        "classifier.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Predict on the test data (20% unseen data)\n",
        "y_pred_test = classifier.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy and other metrics on the test data\n",
        "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "report_test = classification_report(y_test, y_pred_test, target_names=['High', 'Low'])\n",
        "\n",
        "# Output the results for the test set\n",
        "print(f\"Accuracy on the test data (unseen 20%): {accuracy_test}\")\n",
        "print(\"Classification Report on the test data:\")\n",
        "print(report_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pYI-9kyBtYw",
        "outputId": "65e6d136-a0bc-498c-b1ce-5a0f999f66e6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load your data\n",
        "file_path = '/content/drive/MyDrive/Research_Dysgraphia/ANN_modified_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "X = data.drop('severity', axis=1)\n",
        "y = data['severity']\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Define cross-validator and hyperparameter grid for RandomForestClassifier\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],  # Number of trees\n",
        "    'max_depth': [10, 20, 30, None],  # Maximum depth of each tree\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],    # Minimum number of samples required to be at a leaf node\n",
        "    'bootstrap': [True, False]        # Whether bootstrap samples are used when building trees\n",
        "}\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=skf, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Lists to store results\n",
        "metrics_list = []\n",
        "\n",
        "# Perform cross-validation\n",
        "fold = 0\n",
        "for train_index, test_index in skf.split(X_scaled, y):\n",
        "    fold += 1\n",
        "    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]\n",
        "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "\n",
        "    # Apply ADASYN (SMOTE alternative) on each fold training data\n",
        "    smote = ADASYN(sampling_strategy='auto', random_state=42)\n",
        "    X_train_fold_smote, y_train_fold_smote = smote.fit_resample(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Perform grid search to find the best hyperparameters\n",
        "    grid_search.fit(X_train_fold_smote, y_train_fold_smote)\n",
        "\n",
        "    # Train the classifier with the best hyperparameters found\n",
        "    best_classifier = grid_search.best_estimator_\n",
        "\n",
        "    # Predict on the training fold (training accuracy)\n",
        "    y_train_pred = best_classifier.predict(X_train_fold_smote)\n",
        "\n",
        "    # Calculate training accuracy and other metrics\n",
        "    accuracy_train = accuracy_score(y_train_fold_smote, y_train_pred)\n",
        "    report_train = classification_report(y_train_fold_smote, y_train_pred, target_names=['High', 'Low'], output_dict=True)\n",
        "\n",
        "    # Store metrics for each fold\n",
        "    metrics = {\n",
        "        'Fold': fold,\n",
        "        'Training Accuracy': accuracy_train,\n",
        "        'High Precision': report_train['High']['precision'],\n",
        "        'High Recall': report_train['High']['recall'],\n",
        "        'High F1': report_train['High']['f1-score'],\n",
        "        'Low Precision': report_train['Low']['precision'],\n",
        "        'Low Recall': report_train['Low']['recall'],\n",
        "        'Low F1': report_train['Low']['f1-score'],\n",
        "    }\n",
        "    metrics_list.append(metrics)\n",
        "\n",
        "    # Print training metrics for the current fold\n",
        "    print(f\"Training Metrics for Fold {fold}:\")\n",
        "    print(pd.DataFrame([metrics]))\n",
        "\n",
        "    # Print the best hyperparameters for the current fold\n",
        "    print(f\"Best hyperparameters for Fold {fold}: {grid_search.best_params_}\")\n",
        "\n",
        "# Summarize training metrics across all folds\n",
        "all_metrics_df = pd.DataFrame(metrics_list)\n",
        "print(\"\\nSummary of Training Metrics Across All Folds:\")\n",
        "print(all_metrics_df.mean())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
